{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\OPTION\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\OPTION\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\OPTION\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt_tab.zip.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing selesai. Dataset tersimpan di preprocessed_dataset.csv.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "# Unduh stopwords dan tokenizer NLTK jika belum tersedia\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Load dataset (contoh membaca dari file CSV)\n",
    "file_path = \"CISI_ALL.csv\"  # Ganti dengan path file Anda\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "# Fungsi preprocessing\n",
    "def preprocess_text(text):\n",
    "    # 1. Case folding (mengubah teks menjadi huruf kecil)\n",
    "    text = text.lower()\n",
    "    \n",
    "    # 2. Remove special characters, angka, dan tanda baca\n",
    "    text = re.sub(r'[^a-z\\s]', '', text)\n",
    "    \n",
    "    # 3. Tokenisasi\n",
    "    tokens = word_tokenize(text)\n",
    "    \n",
    "    # 4. Remove stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [word for word in tokens if word not in stop_words]\n",
    "    \n",
    "    # 5. Stemming\n",
    "    stemmer = PorterStemmer()\n",
    "    tokens = [stemmer.stem(word) for word in tokens]\n",
    "    \n",
    "    # Gabungkan kembali menjadi teks\n",
    "    preprocessed_text = \" \".join(tokens)\n",
    "    return preprocessed_text\n",
    "\n",
    "# Terapkan preprocessing pada kolom teks\n",
    "data['Preprocessed_Query'] = data['Query'].apply(preprocess_text)\n",
    "\n",
    "# Simpan hasil preprocessing ke file baru\n",
    "output_file = \"preprocessed_dataset.csv\"\n",
    "data.to_csv(output_file, index=False)\n",
    "\n",
    "print(f\"Preprocessing selesai. Dataset tersimpan di {output_file}.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
      "[nltk_data]     C:\\Users\\OPTION\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\OPTION\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\OPTION\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset dengan lemmatization disimpan di lemmatized1_dataset.csv.\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "# Unduh wordnet jika belum tersedia\n",
    "import nltk\n",
    "nltk.download('averaged_perceptron_tagger_eng')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')  # Untuk dukungan bahasa tambahan\n",
    "\n",
    "# Fungsi untuk mendapatkan part of speech (POS) tag yang cocok untuk lemmatization\n",
    "def get_wordnet_pos(word):\n",
    "    \"\"\"Map POS tag to first character used by WordNetLemmatizer.\"\"\"\n",
    "    from nltk.corpus import wordnet\n",
    "    from nltk import pos_tag\n",
    "\n",
    "    tag = pos_tag([word])[0][1][0].upper()\n",
    "    tag_dict = {\"J\": wordnet.ADJ, \"N\": wordnet.NOUN, \"V\": wordnet.VERB, \"R\": wordnet.ADV}\n",
    "    return tag_dict.get(tag, wordnet.NOUN)\n",
    "\n",
    "# Fungsi preprocessing dengan Lemmatization\n",
    "def preprocess_with_lemmatization(text):\n",
    "    # Case folding\n",
    "    text = text.lower()\n",
    "    # Remove special characters\n",
    "    text = re.sub(r'[^a-z\\s]', '', text)\n",
    "    # Tokenize\n",
    "    tokens = word_tokenize(text)\n",
    "    # Remove stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [word for word in tokens if word not in stop_words]\n",
    "    # Lemmatization\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokens = [lemmatizer.lemmatize(word, get_wordnet_pos(word)) for word in tokens]\n",
    "    return \" \".join(tokens)\n",
    "\n",
    "# Terapkan preprocessing pada kolom teks\n",
    "data['Preprocessed_Query'] = data['Query'].apply(preprocess_with_lemmatization)\n",
    "\n",
    "# Simpan hasilnya\n",
    "output_file = \"lemmatized1_dataset.csv\"\n",
    "data.to_csv(output_file, index=False)\n",
    "print(f\"Dataset dengan lemmatization disimpan di {output_file}.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## word embedding dengan gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Word2Vec telah berhasil dilatih dan disimpan ke file.\n",
      "Vektor untuk kata 'library':\n",
      "[ 0.03004955  0.02732495  0.3893054   0.26482025 -0.27534643 -0.39929053\n",
      "  0.49276295  0.43581352 -0.05768473  0.01725278  0.38679412 -0.19372854\n",
      " -0.37777913  0.04664098 -0.12557149 -0.34858975  0.11002731 -0.15013285\n",
      " -0.19472124 -0.34154102  0.29538974  0.00332176  0.14704472  0.06487579\n",
      "  0.16324086 -0.32565227  0.17737181  0.21696636 -0.19167136 -0.11441456\n",
      " -0.17317359  0.00981066  0.19798115 -0.07728899  0.02967602  0.29597965\n",
      "  0.089549    0.05817123  0.05290496 -0.60210294 -0.2613334   0.22473913\n",
      " -0.12272639 -0.02433754 -0.04112124 -0.1941676   0.0311154   0.26438338\n",
      "  0.0153446   0.25202006 -0.5001134  -0.3737789  -0.12476476  0.11853876\n",
      "  0.29325184 -0.13920529  0.06084615  0.03410287  0.01571178  0.22537446\n",
      " -0.11768179  0.15422213  0.0294282  -0.2825325  -0.21761458  0.02165202\n",
      "  0.24223983  0.2450874  -0.27064526  0.09994075  0.04349189  0.21470197\n",
      "  0.5163331  -0.07059516  0.16118188  0.18950875  0.01418779 -0.07133402\n",
      " -0.25276366 -0.30492663  0.16498958  0.06736279 -0.05891474  0.28525153\n",
      " -0.05760299  0.15056701  0.06828789  0.10733981  0.15660231  0.30742472\n",
      "  0.10705989 -0.02554199  0.27682754 -0.08987971  0.39530256  0.10893746\n",
      " -0.12402955 -0.35083905  0.25657386 -0.10089452]\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "import pandas as pd\n",
    "import ast\n",
    "\n",
    "# Load dataset yang telah dipreproses\n",
    "file_path = \"lemmatized1_dataset.csv\"  # Ganti dengan path file preprocessed Anda\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "# Pastikan teks telah diubah menjadi daftar token (jika belum)\n",
    "def text_to_tokens(text):\n",
    "    return text.split()\n",
    "\n",
    "# Konversi kolom Preprocessed_Query menjadi token\n",
    "data['Tokenized_Query'] = data['Preprocessed_Query'].apply(text_to_tokens)\n",
    "\n",
    "# Siapkan data untuk pelatihan Word2Vec\n",
    "sentences = data['Tokenized_Query'].tolist()  # List of tokenized sentences\n",
    "\n",
    "# Train Word2Vec model\n",
    "w2v_model = Word2Vec(\n",
    "    sentences=sentences,     # Data kalimat (tokenized sentences)\n",
    "    vector_size=100,         # Dimensi vektor embedding\n",
    "    window=5,                # Jumlah kata sebelum/sesudah (context window)\n",
    "    min_count=2,             # Minimum kemunculan kata untuk dilibatkan\n",
    "    workers=4,               # Jumlah thread untuk paralelisasi\n",
    "    sg=1                     # Gunakan skip-gram (1) atau CBOW (0)\n",
    ")\n",
    "\n",
    "# Simpan model Word2Vec ke file\n",
    "w2v_model.save(\"word2vec_model.model\")\n",
    "print(\"Model Word2Vec telah berhasil dilatih dan disimpan ke file.\")\n",
    "\n",
    "# Contoh: Mendapatkan vektor untuk sebuah kata\n",
    "word = \"library\"  # Ganti dengan kata yang ingin dilihat\n",
    "if word in w2v_model.wv:\n",
    "    print(f\"Vektor untuk kata '{word}':\\n{w2v_model.wv[word]}\")\n",
    "else:\n",
    "    print(f\"Kata '{word}' tidak ditemukan dalam model Word2Vec.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vektor untuk kata 'library':\n",
      "[ 0.03004955  0.02732495  0.3893054   0.26482025 -0.27534643 -0.39929053\n",
      "  0.49276295  0.43581352 -0.05768473  0.01725278  0.38679412 -0.19372854\n",
      " -0.37777913  0.04664098 -0.12557149 -0.34858975  0.11002731 -0.15013285\n",
      " -0.19472124 -0.34154102  0.29538974  0.00332176  0.14704472  0.06487579\n",
      "  0.16324086 -0.32565227  0.17737181  0.21696636 -0.19167136 -0.11441456\n",
      " -0.17317359  0.00981066  0.19798115 -0.07728899  0.02967602  0.29597965\n",
      "  0.089549    0.05817123  0.05290496 -0.60210294 -0.2613334   0.22473913\n",
      " -0.12272639 -0.02433754 -0.04112124 -0.1941676   0.0311154   0.26438338\n",
      "  0.0153446   0.25202006 -0.5001134  -0.3737789  -0.12476476  0.11853876\n",
      "  0.29325184 -0.13920529  0.06084615  0.03410287  0.01571178  0.22537446\n",
      " -0.11768179  0.15422213  0.0294282  -0.2825325  -0.21761458  0.02165202\n",
      "  0.24223983  0.2450874  -0.27064526  0.09994075  0.04349189  0.21470197\n",
      "  0.5163331  -0.07059516  0.16118188  0.18950875  0.01418779 -0.07133402\n",
      " -0.25276366 -0.30492663  0.16498958  0.06736279 -0.05891474  0.28525153\n",
      " -0.05760299  0.15056701  0.06828789  0.10733981  0.15660231  0.30742472\n",
      "  0.10705989 -0.02554199  0.27682754 -0.08987971  0.39530256  0.10893746\n",
      " -0.12402955 -0.35083905  0.25657386 -0.10089452]\n"
     ]
    }
   ],
   "source": [
    "# load model\n",
    "w2v_model = Word2Vec.load(\"word2vec_model.model\")\n",
    "# Contoh: Mendapatkan vektor untuk sebuah kata\n",
    "word = \"library\"  # Ganti dengan kata yang ingin dilihat\n",
    "if word in w2v_model.wv:\n",
    "    print(f\"Vektor untuk kata '{word}':\\n{w2v_model.wv[word]}\")\n",
    "else:\n",
    "    print(f\"Kata '{word}' tidak ditemukan dalam model Word2Vec.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vektor dokumen pertama:\n",
      "[-0.05298809  0.14883873  0.06761188  0.1690371  -0.02752658 -0.33220217\n",
      "  0.11274979  0.33428666 -0.11857119  0.06671059  0.00186981 -0.18705064\n",
      " -0.08967867 -0.03201508 -0.00272982 -0.19893683  0.02848889 -0.31452104\n",
      " -0.03122917 -0.3317745   0.12665766  0.13904852  0.11094927  0.07895303\n",
      " -0.12379958 -0.1105877  -0.0526808  -0.03794871 -0.12756772 -0.02031747\n",
      "  0.1555505  -0.0566194   0.05236406 -0.01288007 -0.15826122  0.25646824\n",
      "  0.12720212 -0.22771665 -0.10631934 -0.40095854 -0.08801356 -0.08335936\n",
      " -0.01187212  0.03604522  0.17045309  0.01405964 -0.13940947  0.01502349\n",
      " -0.09380537  0.09588465 -0.05034627 -0.18002313 -0.1004464   0.03124702\n",
      " -0.02109029  0.02198052  0.16024938  0.0864702  -0.10872496 -0.00747664\n",
      " -0.03431132  0.1350171  -0.00344313 -0.08790675 -0.2598946   0.13713263\n",
      "  0.07769086  0.1490937  -0.18448609  0.29003373 -0.06146934  0.09239207\n",
      "  0.2964416  -0.01541163  0.14396985  0.10883104 -0.08803122  0.00863197\n",
      " -0.19573492 -0.12197206  0.12411301 -0.01231066 -0.1032595   0.24701741\n",
      "  0.10394359  0.04313584 -0.04380554  0.18514533  0.23563302 -0.0346124\n",
      "  0.08259194  0.10337195  0.08346341 -0.06077793  0.3647801   0.17005944\n",
      "  0.08868837 -0.24006419 -0.03216612 -0.11585768]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def document_vector(doc):\n",
    "    \"\"\"Hitung rata-rata vektor kata-kata dalam dokumen.\"\"\"\n",
    "    words = doc.split()\n",
    "    word_vecs = [w2v_model.wv[word] for word in words if word in w2v_model.wv]\n",
    "    return np.mean(word_vecs, axis=0) if word_vecs else np.zeros(w2v_model.vector_size)\n",
    "\n",
    "# Contoh transformasi untuk dokumen pertama\n",
    "doc_vector = document_vector(data['Preprocessed_Query'][0])\n",
    "print(f\"Vektor dokumen pertama:\\n{doc_vector}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hasil Pencarian:\n",
      "Skor: 0.9465\n",
      "Dokumen: medical library assistance act analysis nlm extramural program cummings martin corn mary e imbalance medical library resource information need health professional lead reexamination mandate national library medicine legislation know medical library assistance act mlaa pass enable nlm initiate program assist nation medical library develop medical library network establishment regional medical library link nlm local institution national library medicine mlaa make available million medical library community competitive grant contract mechanism period july june total project execute resource research development training construction regional medical library publication special scientific project assessment give program impact national library medicine individual medical library aggregate program significantly improve library information service professional health user principal limitation inadequate funding accomplish level originally state objective x\n",
      "\n",
      "Skor: 0.9463\n",
      "Dokumen: cooperation type library stenstrom rh bibliography us traditional typology library start point public school academicresearch special library classification commonly use provide sufficiently clear distinction purpose work public library free resident library district support primarily general public fund tax levy library purpose local public library system public library state library agency include category school library maintain govern board school whether public private parochial school library elementary junior high school senior high school level include academicresearch library include library institution high education public private well library might define either public special widely know research resource junior college library college library university library technical school library include academic library library congress newberry library linda hall library example nonacademic research library special library direct toward make information available people within particular organization fairly welldefined information need business industrial library library nonprofit organization common example distinction research special library always easily make felt case bibliography serious problem create x\n",
      "\n",
      "Skor: 0.9462\n",
      "Dokumen: philadelphia project benford jq objective research determine actual requirement library resource elementary secondary grade student evaluate exist library resource term student need national standard basis information outline respective role school library public library provide need resource develop joint planning x\n",
      "\n",
      "Skor: 0.9448\n",
      "Dokumen: academic status college university librarian problem prospect smith eldred academic librarian archive deserve full academic status cause change bureaucratic structure library library education provide professional service scholarly level x\n",
      "\n",
      "Skor: 0.9437\n",
      "Dokumen: regional network ohio college library center kilgour fg ohio college library center develop computerize useroriented library system improve efficiency library use operation increase availability library resource within region facilitate evolution new easy access information library center conceive task merely mechanization library procedure past immediate future principal academic objective center increase availability library resource use educational research program college university throughout ohio distant objective enable library participate actively program instruction research institution passive service function library developed past century prove inadequate meet present demand make library increasingly inadequate future perstudent cost library rise somewhat twice rapidly unitcost rise general economy therefore principal economic goal ohio college library center decelerate rate rise perstudent cost rate increase approximate economy whole x\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "\n",
    "# Fungsi untuk membuat rata-rata vektor kata dalam dokumen\n",
    "def average_vector(text, model):\n",
    "    \"\"\"Menghitung rata-rata vektor kata dalam teks berdasarkan model Word2Vec.\"\"\"\n",
    "    words = text.split()\n",
    "    vectors = []\n",
    "    for word in words:\n",
    "        if word in model.wv:  # Hanya gunakan kata yang ada di Word2Vec\n",
    "            vectors.append(model.wv[word])\n",
    "    if len(vectors) == 0:\n",
    "        return np.zeros(model.vector_size)  # Jika tidak ada kata yang cocok, kembalikan nol\n",
    "    return np.mean(vectors, axis=0)\n",
    "\n",
    "# Preprocess semua dokumen dan hitung vektornya\n",
    "def preprocess_and_vectorize_documents(documents, model):\n",
    "    \"\"\"Preprocessing dan konversi dokumen menjadi vektor.\"\"\"\n",
    "    doc_vectors = []\n",
    "    for doc in documents:\n",
    "        processed_text = preprocess_with_lemmatization(doc)  # Gunakan fungsi preprocess_with_lemmatization yang telah dibuat\n",
    "        doc_vectors.append(average_vector(processed_text, model))\n",
    "    return doc_vectors\n",
    "\n",
    "# Fungsi untuk mencari dokumen relevan\n",
    "def search_documents(query, model, doc_vectors, documents, top_n=5):\n",
    "    \"\"\"Melakukan pencarian dokumen berdasarkan query.\"\"\"\n",
    "    # preprocess_with_lemmatization dan vektorkan query\n",
    "    processed_query = preprocess_with_lemmatization(query)  # Preprocess query\n",
    "    query_vector = average_vector(processed_query, model)\n",
    "    \n",
    "    # Hitung kesamaan cosine antara query dan semua dokumen\n",
    "    similarities = []\n",
    "    for i, doc_vector in enumerate(doc_vectors):\n",
    "        sim = cosine_similarity([query_vector], [doc_vector])[0][0]\n",
    "        similarities.append((i, sim))\n",
    "    \n",
    "    # Urutkan dokumen berdasarkan kesamaan (descending)\n",
    "    similarities = sorted(similarities, key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    # Ambil top-n dokumen yang paling relevan\n",
    "    top_docs = similarities[:top_n]\n",
    "    \n",
    "    # Tampilkan hasil\n",
    "    results = []\n",
    "    for idx, score in top_docs:\n",
    "        results.append((documents[idx], score))\n",
    "    return results\n",
    "\n",
    "# Contoh Implementasi\n",
    "# 1. List semua dokumen\n",
    "documents = data['Preprocessed_Query'].tolist()  # Kolom hasil preprocess\n",
    "\n",
    "# 2. Hitung vektor dokumen\n",
    "doc_vectors = preprocess_and_vectorize_documents(documents, w2v_model)\n",
    "\n",
    "# 3. Masukkan query dan cari dokumen\n",
    "query = \"library management\"\n",
    "results = search_documents(query, w2v_model, doc_vectors, documents)\n",
    "\n",
    "# 4. Print hasil pencarian\n",
    "print(\"Hasil Pencarian:\")\n",
    "for doc, score in results:\n",
    "    print(f\"Skor: {score:.4f}\\nDokumen: {doc}\\n\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
