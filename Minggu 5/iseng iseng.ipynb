{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\OPTION\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\OPTION\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\OPTION\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import logging\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, confusion_matrix\n",
    "import streamlit as st\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Penggunaan CPU: 43.6%\n",
      "Penggunaan Memori: 77.1%\n",
      "Penggunaan Memori melebihi batas! Menghentikan proses...\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 9\u001b[0m\n\u001b[0;32m      6\u001b[0m file_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCISI_ALL.csv\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# Memuat dataset\u001b[39;00m\n\u001b[1;32m----> 9\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[43mload_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m# Menampilkan beberapa baris pertama dataset\u001b[39;00m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28mprint\u001b[39m(data\u001b[38;5;241m.\u001b[39mhead())\n",
      "Cell \u001b[1;32mIn[1], line 2\u001b[0m, in \u001b[0;36mload_dataset\u001b[1;34m(file_path)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_dataset\u001b[39m(file_path):\n\u001b[1;32m----> 2\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mpd\u001b[49m\u001b[38;5;241m.\u001b[39mread_csv(file_path)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "def load_dataset(file_path):\n",
    "    return pd.read_csv(file_path)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Path ke file dataset yang diunggah\n",
    "    file_path = 'CISI_ALL.csv'\n",
    "    \n",
    "    # Memuat dataset\n",
    "    data = load_dataset(file_path)\n",
    "    \n",
    "    # Menampilkan beberapa baris pertama dataset\n",
    "    print(data.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PREPOCESING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\OPTION\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\OPTION\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing selesai. Dataset tersimpan di preprocessed_dataset.csv.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "# Unduh stopwords dan tokenizer NLTK jika belum tersedia\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Load dataset (contoh membaca dari file CSV)\n",
    "file_path = \"CISI_ALL.csv\"  # Ganti dengan path file Anda\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "# Fungsi preprocessing\n",
    "def preprocess_text(text):\n",
    "    # 1. Case folding (mengubah teks menjadi huruf kecil)\n",
    "    text = text.lower()\n",
    "    \n",
    "    # 2. Remove special characters, angka, dan tanda baca\n",
    "    text = re.sub(r'[^a-z\\s]', '', text)\n",
    "    \n",
    "    # 3. Tokenisasi\n",
    "    tokens = word_tokenize(text)\n",
    "    \n",
    "    # 4. Remove stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [word for word in tokens if word not in stop_words]\n",
    "    \n",
    "    # 5. Stemming\n",
    "    stemmer = PorterStemmer()\n",
    "    tokens = [stemmer.stem(word) for word in tokens]\n",
    "    \n",
    "    # Gabungkan kembali menjadi teks\n",
    "    preprocessed_text = \" \".join(tokens)\n",
    "    return preprocessed_text\n",
    "\n",
    "# Terapkan preprocessing pada kolom teks\n",
    "data['Preprocessed_Query'] = data['Query'].apply(preprocess_text)\n",
    "\n",
    "# Simpan hasil preprocessing ke file baru\n",
    "output_file = \"preprocessed_dataset.csv\"\n",
    "data.to_csv(output_file, index=False)\n",
    "\n",
    "print(f\"Preprocessing selesai. Dataset tersimpan di {output_file}.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                  Preprocessed_Query  \\\n",
      "0  edit dewey decim classif comaromi jp present s...   \n",
      "1  use made technic librari slater report analysi...   \n",
      "2  two kind power essay bibliograph control wilso...   \n",
      "3  system analysi univers librari final report re...   \n",
      "4  librari manag game report research project bro...   \n",
      "\n",
      "                                              Tokens  \n",
      "0  [101, 10086, 20309, 11703, 5714, 2465, 10128, ...  \n",
      "1  [101, 2224, 2081, 6627, 8713, 5622, 10024, 308...  \n",
      "2  [101, 2048, 2785, 2373, 9491, 12170, 16558, 36...  \n",
      "3  [101, 2291, 20302, 7274, 2072, 4895, 16402, 20...  \n",
      "4  [101, 5622, 10024, 3089, 24951, 2290, 2208, 31...  \n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Tokenize the cleaned text\n",
    "data['Tokens'] = data['Preprocessed_Query'].apply(lambda x: tokenizer.encode(x, truncation=True, padding='max_length', max_length=128))\n",
    "\n",
    "# Display the result\n",
    "print(data[['Preprocessed_Query', 'Tokens']].head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train texts: 254     [101, 15535, 3191, 6090, 2669, 17294, 1038, 22...\n",
      "1066    [101, 2715, 2291, 2470, 5248, 7155, 3120, 8654...\n",
      "638     [101, 3465, 3466, 23408, 4747, 4904, 12367, 22...\n",
      "799     [101, 3930, 21183, 4014, 27885, 19454, 2229, 2...\n",
      "380     [101, 9345, 7630, 12367, 14262, 7903, 4031, 23...\n",
      "Name: Tokens, dtype: object\n",
      "Test texts: 892     [101, 10424, 2063, 4226, 12273, 4487, 3367, 30...\n",
      "1105    [101, 1041, 26989, 6895, 12367, 14262, 7903, 2...\n",
      "413     [101, 2010, 29469, 5622, 10024, 3089, 2530, 20...\n",
      "522     [101, 3465, 4842, 14192, 2006, 4115, 2489, 182...\n",
      "1036    [101, 2646, 5248, 14833, 3089, 4012, 23041, 93...\n",
      "Name: Tokens, dtype: object\n",
      "Train labels: 254      255\n",
      "1066    1067\n",
      "638      639\n",
      "799      800\n",
      "380      381\n",
      "Name: ID, dtype: int64\n",
      "Test labels: 892      893\n",
      "1105    1106\n",
      "413      414\n",
      "522      523\n",
      "1036    1037\n",
      "Name: ID, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split the dataset\n",
    "train_texts, test_texts, train_labels, test_labels = train_test_split(\n",
    "    data['Tokens'], data['ID'], test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Display the split dataset\n",
    "print(\"Train texts:\", train_texts.head())\n",
    "print(\"Test texts:\", test_texts.head())\n",
    "print(\"Train labels:\", train_labels.head())\n",
    "print(\"Test labels:\", test_labels.head())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train inputs: tensor([[  101, 15535,  3191,  ...,     0,     0,     0],\n",
      "        [  101,  2715,  2291,  ...,     0,     0,     0],\n",
      "        [  101,  3465,  3466,  ...,     0,     0,     0],\n",
      "        ...,\n",
      "        [  101,  4012, 23041,  ...,     0,     0,     0],\n",
      "        [  101, 28667,  2239,  ...,     0,     0,     0],\n",
      "        [  101,  9345,  7630,  ...,     0,     0,     0]])\n",
      "Train labels: tensor([ 255, 1067,  639,  ..., 1295,  861, 1127])\n",
      "Test inputs: tensor([[  101, 10424,  2063,  ...,     0,     0,     0],\n",
      "        [  101,  1041, 26989,  ...,     0,     0,     0],\n",
      "        [  101,  2010, 29469,  ...,     0,     0,     0],\n",
      "        ...,\n",
      "        [  101,  5082,  6254,  ...,  4765, 10128,   102],\n",
      "        [  101,  2128, 20414,  ...,     0,     0,     0],\n",
      "        [  101,  4503,  3465,  ...,     0,     0,     0]])\n",
      "Test labels: tensor([ 893, 1106,  414,  523, 1037,  615,  219, 1161,  650,  888,  577, 1253,\n",
      "        1062,  568, 1109, 1114,  169, 1103, 1121,   68, 1041,  454,  671, 1095,\n",
      "         193,  124,  416,  278,  434, 1318,  185,  555, 1174,   77,  907,  675,\n",
      "        1399,  375, 1033,  260,   52,  245, 1102,  582,  680, 1133, 1221,   50,\n",
      "         592, 1270, 1250, 1139,  491, 1025,  812,  142,  847,  926,  773,  533,\n",
      "          30, 1300,   66,  766, 1348,  176,  395,  620, 1132,  116,  895, 1222,\n",
      "         428,  737,  899,  683,  991,  719,   45,   60,  364,  399,  423,  495,\n",
      "         417,  585,  271,  352, 1079, 1345,  837,  644,  275,  323,   79,  923,\n",
      "         276, 1266,  221,  317, 1007, 1241,   71,   16, 1165, 1321,  755,  721,\n",
      "        1048,  629,  637,  465,  856, 1398,  383, 1429,  789, 1341, 1217,  333,\n",
      "         544,  204, 1135, 1415,  430, 1273,  747,  590,  839, 1294,  591,  586,\n",
      "        1202,  199,  917,  995, 1229,   44, 1034,  692,  232,  527,  102, 1093,\n",
      "         412, 1393,  908,  783,  953,  621,  670,  129,  530,  911,  999, 1451,\n",
      "        1164,  248,  529,  734,   33,  635, 1028, 1054,  576,  844,  298,  262,\n",
      "        1047,  940,  310,  164, 1010,  600, 1176,  372,  240,  882, 1334, 1359,\n",
      "        1157,  813, 1332,  426, 1055,   64,  818, 1395, 1307,  916,  433,  694,\n",
      "         967, 1171, 1043,  608, 1090,   31,   57, 1179,  452,  559,  366,  424,\n",
      "         959,  807, 1134,  980,  355, 1357,  307,  797,  100,  756,  272,  241,\n",
      "         598,  947,  359,  985,  778,  708,  599,  875,  931,  340, 1280, 1433,\n",
      "        1042,  712, 1050,  311,  572,  918,  603,  290,  429, 1086,  209,  681,\n",
      "         354,  325,  238,   59, 1160,  536,  368,  696,   24,  108, 1431, 1019,\n",
      "         362, 1002,  539, 1422, 1178, 1292,  782, 1448,  775,  673,  234,  427,\n",
      "         197, 1227,   82, 1369, 1126,  112, 1244,  745,  938,  345, 1233,  866,\n",
      "        1089,  351,  589, 1428,  949, 1450,  990,  678,  479, 1272, 1411,  480,\n",
      "        1362,  803,  652,  723])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "train_inputs = torch.tensor(train_texts.tolist())\n",
    "train_labels = torch.tensor(train_labels.tolist())\n",
    "test_inputs = torch.tensor(test_texts.tolist())\n",
    "test_labels = torch.tensor(test_labels.tolist())\n",
    "\n",
    "# Display the results\n",
    "print(\"Train inputs:\", train_inputs)\n",
    "print(\"Train labels:\", train_labels)\n",
    "print(\"Test inputs:\", test_inputs)\n",
    "print(\"Test labels:\", test_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "from transformers import BertForSequenceClassification, AdamW\n",
    "\n",
    "# Load the pre-trained model\n",
    "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=len(data['ID'].unique()))\n",
    "\n",
    "# Define optimizer\n",
    "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
    "\n",
    "# Training loop (simplified)\n",
    "model.train()\n",
    "for epoch in range(3):  # Number of epochs\n",
    "    optimizer.zero_grad()\n",
    "    outputs = model(train_inputs, labels=train_labels)\n",
    "    loss = outputs.loss\n",
    "    loss.backward()\n",
    "    optimizer.step()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
